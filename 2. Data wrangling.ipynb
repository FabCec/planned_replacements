{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SMART metrics are the foundations of our predictive maintenance policy and, since their meaning is model dependent, the policy we derive will be different for each of them. As a first step, we read the dataset from the zipped repositories in [Backblaze](https://www.backblaze.com/b2/hard-drive-test-data.html) and identify the models which have enough entries to be suitable for the analysis.\n",
    "\n",
    "This notebook provides a preliminary description of the dataset relative to each model. We will read the data from the zipped online repository instead of downloading the entire datasets due to their large dimensions. After the preliminary analysis we will focus on specific models, read the relative data, and save only the meaningful features (more about what we consider meaningful below).\n",
    "\n",
    "** Note: To correctly run this notebook a good internet connection is required. By the end of the notebook the data relative to a specific model will be saved on the laptop. **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# String needed to access the data in the zipfiles stored at https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data\n",
    "\n",
    "url_15 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_2015.zip'\n",
    "start_str_15 = '2015/'\n",
    "\n",
    "url_16_q1 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q1_2016.zip'\n",
    "start_str_16_q1 = 'data_Q1_2016/'\n",
    "\n",
    "url_16_q2 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q2_2016.zip'\n",
    "start_str_16_q2 = 'data_Q2_2016/'\n",
    "\n",
    "url_16_q3 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q3_2016.zip'\n",
    "start_str_16_q3 = 'data_Q3_2016/'\n",
    "\n",
    "url_16_q4 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q4_2016.zip'\n",
    "start_str_16_q4 = ''\n",
    "\n",
    "url_17_q1 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q1_2017.zip'\n",
    "start_str_17_q1 = ''\n",
    "\n",
    "url_17_q2 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q2_2017.zip'\n",
    "start_str_17_q2 = ''\n",
    "\n",
    "url_17_q3 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q3_2017.zip'\n",
    "start_str_17_q3 = ''\n",
    "\n",
    "url_17_q4 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q4_2017.zip'\n",
    "start_str_17_q4 = 'data_Q4_2017/'\n",
    "\n",
    "list_url = [[[url_15,start_str_15]], \n",
    "            [[url_16_q1,start_str_16_q1], [url_16_q2,start_str_16_q2], [url_16_q3,start_str_16_q3], [url_16_q4,start_str_16_q4]], \n",
    "            [[url_17_q1,start_str_17_q1], [url_17_q2,start_str_17_q2], [url_17_q3,start_str_17_q3], [url_17_q4,start_str_17_q4]]\n",
    "           ]\n",
    "\n",
    "list_keys = [15, 16, 17]\n",
    "\n",
    "dict_zipfile = dict(zip(list_keys,list_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section A of this notebook is used to grasp the structure of the dataset with respect to the various model.\n",
    "\n",
    "** If interested in a specific model, skip to Section B ** and run the rest of the notebook after specifying the **ModelName** variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Number of failures per model\n",
    "\n",
    "We will list the number of unique HDDs per model and how many of them failed in the three-year period considered. We reported only those models with at least 1000 unique HDDs and ordered them by fraction of failed HDDs. \n",
    "\n",
    "Note that the ratio is always below 10%, we thus have to face **class imbalance** as it is typical for anomaly detection problems. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions needed to read, analyze, and save the dataframes associated to the various models\n",
    "\n",
    "def read_entries_and_failures( url, start_string = ''):\n",
    "    '''This function reads the zip file located at url, open it, read the columns needed \n",
    "    to capture the per model number of entries and failures'''\n",
    "    ListDF = []\n",
    "    r = requests.get(url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    files = [name for name in z.namelist() if (name.endswith('.csv')) & (name.startswith(start_string))]\n",
    "\n",
    "    for file in files : \n",
    "        data = pd.read_csv(z.open(file))\n",
    "        data2 = data[['date','serial_number', 'model', 'failure']]\n",
    "        ListDF.append(data2)\n",
    "        \n",
    "    df = pd.concat(ListDF, ignore_index = True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "    \n",
    "def per_model_entries_and_failures (df) :   \n",
    "    '''Given a dataframe with the observations and the columns 'model', 'serial_number', and 'failure, \n",
    "    return a dataframe with number of failures and unique serial numbers'''\n",
    "    FailuresPerModel = df.groupby('model')['failure'].sum()\n",
    "    EntriesPerModel = df.groupby('model')['serial_number'].unique()\n",
    "    ModelsDetail = pd.concat([EntriesPerModel,FailuresPerModel],axis=1)\n",
    "    \n",
    "    return ModelsDetail\n",
    "    \n",
    "    \n",
    "    \n",
    "def save_yearly_entries_and_failures( dict_url, year ) :\n",
    "    '''This function read the datasets located at dict_url associated to a specific year, it print the number of\n",
    "    entries for that year, and save the dataframe containing the number of entries and failures per model'''\n",
    "    list_df = []\n",
    "\n",
    "    for url, start_string in dict_url[year] :\n",
    "        df = read_entries_and_failures(url, start_string)  \n",
    "        list_df = list_df + [df]\n",
    "        \n",
    "    if len(dict_zipfile[year]) > 1 :\n",
    "        df = pd.concat(list_df, ignore_index = True)\n",
    "        \n",
    "    return per_model_entries_and_failures(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the dataset relative to years 2015-2017 and save the number of unique entries and failures per model\n",
    "\n",
    "ModelsDetail_15 = save_yearly_entries_and_failures( dict_zipfile, 15 )\n",
    "ModelsDetail_16 = save_yearly_entries_and_failures( dict_zipfile, 16 )\n",
    "ModelsDetail_17 = save_yearly_entries_and_failures( dict_zipfile, 17 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######### TO BE SKIPPED IF INTERESTED ONLY IN A SPECIFIC MODEL #########\n",
    "\n",
    "## Combine the dataframe extracted in a unique dataframe\n",
    "\n",
    "ListDF_raw = [ModelsDetail_15.add_suffix('_15'), ModelsDetail_16.add_suffix('_16'), ModelsDetail_17.add_suffix('_17')]\n",
    "DfTot = pd.concat(ListDF_raw, axis = 1) \n",
    "\n",
    "# If the failure entry is equal to NaN, the value is replaced with a 0\n",
    "DfTot.failure_15 = DfTot.failure_15.fillna(0) \n",
    "DfTot.failure_16 = DfTot.failure_16.fillna(0) \n",
    "DfTot.failure_17 = DfTot.failure_17.fillna(0) \n",
    "\n",
    "# Compute the total number of failures in the period 2015-2017\n",
    "DfTot['failure_tot'] = DfTot.failure_15 + DfTot.failure_16 + DfTot.failure_17\n",
    "del DfTot['failure_15']\n",
    "del DfTot['failure_16']\n",
    "del DfTot['failure_17']\n",
    "\n",
    "# Merge the serial_numbers that appear in the period 2015-2017\n",
    "DfTot['serial_number_tot'] = [np.hstack((DfTot.loc[mod,'serial_number_15'], DfTot.loc[mod,'serial_number_16'], DfTot.loc[mod,'serial_number_17'])) for mod in DfTot.index]\n",
    "\n",
    "del DfTot['serial_number_15']\n",
    "del DfTot['serial_number_16']\n",
    "del DfTot['serial_number_17']\n",
    "\n",
    "# Count the unique entries in the serial_number column (use the set structure to get rid of the duplicates)\n",
    "DfTot['entries_tot'] = [len(set(DfTot.loc[mod,'serial_number_tot'])) for mod in DfTot.index]\n",
    "\n",
    "del DfTot['serial_number_tot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>failure_tot</th>\n",
       "      <th>entries_tot</th>\n",
       "      <th>ratio_entry_failures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>WDC WD30EFRX</th>\n",
       "      <td>122.0</td>\n",
       "      <td>1261</td>\n",
       "      <td>0.096749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST3000DM001</th>\n",
       "      <td>106.0</td>\n",
       "      <td>1170</td>\n",
       "      <td>0.090598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST4000DM000</th>\n",
       "      <td>2590.0</td>\n",
       "      <td>36700</td>\n",
       "      <td>0.070572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST31500541AS</th>\n",
       "      <td>112.0</td>\n",
       "      <td>1693</td>\n",
       "      <td>0.066155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hitachi HDS723030ALA640</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1018</td>\n",
       "      <td>0.043222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hitachi HDS722020ALA330</th>\n",
       "      <td>152.0</td>\n",
       "      <td>4683</td>\n",
       "      <td>0.032458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST6000DX000</th>\n",
       "      <td>60.0</td>\n",
       "      <td>1938</td>\n",
       "      <td>0.030960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hitachi HDS5C3030ALA630</th>\n",
       "      <td>96.0</td>\n",
       "      <td>4608</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hitachi HDS5C4040ALE630</th>\n",
       "      <td>42.0</td>\n",
       "      <td>2660</td>\n",
       "      <td>0.015789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST8000DM002</th>\n",
       "      <td>141.0</td>\n",
       "      <td>10029</td>\n",
       "      <td>0.014059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HGST HMS5C4040ALE640</th>\n",
       "      <td>97.0</td>\n",
       "      <td>8661</td>\n",
       "      <td>0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HGST HMS5C4040BLE640</th>\n",
       "      <td>135.0</td>\n",
       "      <td>16306</td>\n",
       "      <td>0.008279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST8000NM0055</th>\n",
       "      <td>88.0</td>\n",
       "      <td>14510</td>\n",
       "      <td>0.006065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST10000NM0086</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1225</td>\n",
       "      <td>0.002449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST12000NM0007</th>\n",
       "      <td>17.0</td>\n",
       "      <td>7244</td>\n",
       "      <td>0.002347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         failure_tot  entries_tot  ratio_entry_failures\n",
       "WDC WD30EFRX                   122.0         1261              0.096749\n",
       "ST3000DM001                    106.0         1170              0.090598\n",
       "ST4000DM000                   2590.0        36700              0.070572\n",
       "ST31500541AS                   112.0         1693              0.066155\n",
       "Hitachi HDS723030ALA640         44.0         1018              0.043222\n",
       "Hitachi HDS722020ALA330        152.0         4683              0.032458\n",
       "ST6000DX000                     60.0         1938              0.030960\n",
       "Hitachi HDS5C3030ALA630         96.0         4608              0.020833\n",
       "Hitachi HDS5C4040ALE630         42.0         2660              0.015789\n",
       "ST8000DM002                    141.0        10029              0.014059\n",
       "HGST HMS5C4040ALE640            97.0         8661              0.011200\n",
       "HGST HMS5C4040BLE640           135.0        16306              0.008279\n",
       "ST8000NM0055                    88.0        14510              0.006065\n",
       "ST10000NM0086                    3.0         1225              0.002449\n",
       "ST12000NM0007                   17.0         7244              0.002347"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### TO BE SKIPPED IF INTERESTED ONLY IN A SPECIFIC MODEL #########\n",
    "\n",
    "## Compute the percentage of failed HDDs and order the models in descending order w.r.t. such percentage\n",
    "\n",
    "DfTot['ratio_entry_failures'] = DfTot['failure_tot'].divide(DfTot['entries_tot'], axis = 'rows')\n",
    "DfTot.sort_values('ratio_entry_failures', inplace=True, ascending =False)\n",
    "\n",
    "# Display only the models with at least 1000 unique HDDs\n",
    "\n",
    "DfTot[DfTot['entries_tot'] > 1000]\n",
    "DfTot.to_csv('EntriesFailureRatio.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Model-based data cleaning\n",
    "\n",
    "In the following we will focus on the model **Hitachi HDS722020ALA330** and report only the result associated to such model. The code is fully automated and the same analysis can be performed for the other models as well by replacing the value assigned to **ModelName**. \n",
    "\n",
    "We decided to present the results for the model **Hitachi HDS722020ALA330** being it the average case. It has about 5000 entries and 3% failures, where the former ranges in between 1000 (to be suitable for analysis) and 15000 (minus one outlier at 36000) and the latter ranges from 0.2% to 9.6%.\n",
    "\n",
    "Observe that since we consider a unique model, we can immediately drop the columns 'Model' and 'Capacity' from the dataset. For every SMART metric we have both the raw and the normalized value, for the sake of simplicity we will work only with the normalized quantities. Thus, the original dataset (with 95 columns) is immediately reduced to one with 48 columns (date, serial number, failure, and the normalized SMART metrics). We further reduce the dataset by dropping the **non-informative** SMART metrics. In particular, given a specific model some of the SMART metrics are \n",
    "absent and others do not vary significantly and thus not provide meaningful information. While the former are easy to identify, we need to provide a way to judge which metrics belong to the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Select the model you want to work with. (ex:'Hitachi HDS722020ALA330', 'ST8000DM002', ...) ####\n",
    "\n",
    "ModelName = 'Hitachi HDS722020ALA330' \n",
    "#ModelName = 'ST8000DM002' \n",
    "#ModelName = 'HGST HMS5C4040BLE640' \n",
    "#ModelName = 'WDC WD30EFRX' \n",
    "#ModelName = 'ST12000NM0007' \n",
    "#ModelName = 'Hitachi HDS5C3030ALA630' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.1. Non-informative SMART metrics\n",
    "\n",
    "Given a SMART metric, we claim that it is non-informative if it does not vary sufficiently over the dataset and is never below the typical value. \n",
    "We came up with the following method to filter-out non-informative SMART metrics. Assume that a SMART metric takes values $\\mathbf{x} = x_1, \\ldots, x_N$ for a specific model, we can compute \n",
    "\n",
    "- The variance $v(\\mathbf{x}) = \\text{Var}\\big( x_1, \\ldots, x_N \\big)$;\n",
    "- The range $r(\\mathbf{x}) = \\max\\big( x_1, \\ldots, x_N \\big) - \\min\\big( x_1, \\ldots, x_N \\big)$;\n",
    "- The minimum $m(\\mathbf{x}) = \\min\\big( x_1, \\ldots, x_N \\big)$.\n",
    "\n",
    "We claim that a metric is non-informative if\n",
    "\n",
    "$$\n",
    " \\Big( m(\\mathbf{x}) \\geq 100 \\Big) \\text{ or } \\Big( \\big( r(\\mathbf{x}) < T_r \\big) \\text{ and } \\big( v(\\mathbf{x}) < T_v \\big) \\Big) \n",
    "$$\n",
    "\n",
    "with $T_r$ and $T_v$ chosen thresholds. We selected $T_r = 15$ and $T_v = 1$.\n",
    "\n",
    "Note that if $m(\\mathbf{x}) \\geq 100$ it means that the metric never takes values below the typical and thus do not provide information of eventual \n",
    "malfunctions. To capture the variability of the metric, we believe that the variance alone would not be sufficient, \n",
    "in fact in some cases the SMART metric may drop abruptly just before a failure. In such\n",
    "case the large majority of the values for the metric would be close to a typical value and the variance would be too low due to the small quantity of unusual values.\n",
    "\n",
    "The filter above is coded in the function **is_relevant** below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_relevant(col, thresh_diff = 15, thresh_var = 1) :\n",
    "    ''' Given a column of the DataFrame, decide whether it is not informative (return False) \n",
    "    or it might be (return True)'''\n",
    "    diff = col.max() - col.min()\n",
    "    var = col.var()\n",
    "    mincol = col.min()\n",
    "    if mincol > 99 :                                # Check whether the metric is ever below the typical value 100\n",
    "        return False\n",
    "    if (diff > thresh_diff) | (var > thresh_var):   # Check whether the metric varies enough\n",
    "        return True\n",
    "    else :\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function __is_relevant__ provides a deterministic filter that could be considered as a preliminary feature selection. This does not exclude a further feature selection procedure exploiting machine learning to be used later on. \n",
    "\n",
    "In particular, more than saying whether a SMART metric is correlated with failures, this function simply excludes metrics that certainly aren't, i.e., the metrics that are never below the typical value or do not show significant variation in their normalized value across the dataset.\n",
    "\n",
    "Note that the filter can be made more or less inclusive by adjusting the values of the thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The following functions are needed in order to build a mask for any specific model. The mask is a filter that selects\n",
    "## only the columns associated to potentially informative SMART metrics \n",
    "## (the mask excludes the metrics that are non-informative and those that are absent for a specific model)\n",
    "\n",
    "# (1) Read the dataset associated to a model and a specific mask (only columns specified in rel_cols).\n",
    "def read_model_and_rel_cols (mod_name, rel_cols, DictUrl, year):\n",
    "    ''' This function reads the zip file of the selected year, open it, read the columns associated to a model and \n",
    "    the relevant columns '''\n",
    "    ListDF = []\n",
    "    \n",
    "    for url, start_string in DictUrl[year] :\n",
    "        r = requests.get(url)\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        files = [name for name in z.namelist() if (name.endswith('.csv')) & (name.startswith(start_string))]\n",
    "        for file in files : \n",
    "            data = pd.read_csv(z.open(file), parse_dates = [0])\n",
    "            data2 = data[data['model'] == mod_name]\n",
    "            data3 = data2.iloc[:,rel_cols]\n",
    "            ListDF.append(data3)\n",
    "    \n",
    "    df = pd.concat(ListDF, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# (2) Return the SMART metrics with are present (not NaN values) and are not non-informative (is_relevant returned True).\n",
    "def find_relevant_cols(df, thresh_diff = 15, thresh_var = 1) :\n",
    "    ''' For the various SMART metrics return True only if the metric is considered by a specific model, i.e. the \n",
    "    relative values are not NaN and if the metric is relevant. Return the 45 boolean values list.'''\n",
    "    non_na_cols = (df.isnull().sum() == 0)\n",
    "        \n",
    "    res_bool = []\n",
    "        \n",
    "    for col in df.columns :\n",
    "        if non_na_cols[col] == True :\n",
    "            res_bool = res_bool + [is_relevant(df[col], thresh_diff, thresh_var)]\n",
    "        else :\n",
    "            res_bool = res_bool + [False]\n",
    "    return res_bool\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Build the mask for the model specified in ModelName.\n",
    "\n",
    "# normalized_cols has 'False' in correspondence of a column with a raw value and 'True' of a col with a normalized value\n",
    "normalized_cols = [True, False]       # The SMART metrics columns are ordered as Normalized, Raw, Normalized, Raw, ...\n",
    "normalized_cols = normalized_cols*45  # ... and there are 45 SMART metrics\n",
    "\n",
    "# The column names are ordered as [date, serial number, model, capacity, failure] + SMART metrics\n",
    "cols = [True, True, False, False, True] + normalized_cols \n",
    "\n",
    "df_year = {}\n",
    "mask = [False]*45\n",
    "\n",
    "for year in [15,16,17] :\n",
    "    # Call (1) : Read all the columns with normalized SMART metrics\n",
    "    df_year[year] = read_model_and_rel_cols (ModelName, cols, dict_zipfile, year)\n",
    "    # Call (2) : Find the relevant columns with the dataframe df obtained with the previous call\n",
    "    mask_year = find_relevant_cols(df_year[year].iloc[:,3:], 15, 1) \n",
    "    mask = [ x | y for (x,y) in zip(mask, mask_year)]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2. Model-based data filtering\n",
    "\n",
    "We now use the filter created to select only the relevant columns for the model in ModelName. The use of the mask allows a drastic reduction in the dimensions of the dataset making it way more manageable while retaining most the information that could lead to the early prediction of an imminent failure. \n",
    "\n",
    "**For the model Hitachi HDS722020ALA330 we drop most of the metrics and keep only the columns associated to the normalized SMART metrics number 1, 5, 7, 8, 192, 193, and 196**, i.e., 7 out of the initial 45 metrics. \n",
    "\n",
    "We now save the model-based filtered dataframe in a dataset that will be used in the following notebooks to analyze the relation between SMART metrics and time-to-failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Path to the folder in which the datset will be stored\n",
    "folder_name_model = ModelName.replace(' ', '_')\n",
    "folder_name = 'Data/' + folder_name_model\n",
    "\n",
    "# Check whether the folder exists and create it if it does not. \n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# We drop the columns relative to model, capacity, and non-informative SMART metrics\n",
    "cols_chosen = [True, True, True] + mask      # [date, serial number, failure] + chosen SMART metrics     \n",
    "    \n",
    "# Save one separate dataframe per year\n",
    "for year in [15,16,17] :\n",
    "    df_year_chosen = df_year[year].iloc[:, cols_chosen]\n",
    "    df_year_chosen.to_csv(folder_name + '/Model_' + str(year) + '.csv')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
