{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  What hard-disk model shall we look at?\n",
    "\n",
    "### The SMART metrics present in the dataset are model-dependent, thus the various models have to be treated differently. In this project we will focus on a single model. \n",
    "\n",
    "With this script we aim to capture the most convenient model to focus on. We deduce that the model ST4000DM000 is the most used throughout 2015, 2016, and 2017 with 37600 differend hard disks of which about 7% have failed.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We load the datasets and extract the relevant columns and save them in ModelsDetail_15, ModelsDetail_16, and ModelsDetail_17. The relevant columns are :\n",
    "\n",
    "date : The time stamp of the observations.\n",
    "\n",
    "serial_number : Uniquely identifies a hard-disk. It is used to determine the number of distinct entries.\n",
    "\n",
    "model : Identifies the model of the hard-disk. \n",
    "\n",
    "failure : A value in {0,1}. When a 1 is present, the hard disk has failed on the specific date and is removed from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReadFiles_EntriesFailure( url, start_string = ''):\n",
    "    '''This function reads the zip file located at url, open it, read the columns needed to capture the ratio failures/entries'''\n",
    "    ListDF = []\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "\n",
    "    files = [name for name in z.namelist() if (name.endswith('.csv')) & (name.startswith(start_string))]\n",
    "\n",
    "    for file in files : \n",
    "        data = pd.read_csv(z.open(file))\n",
    "        data2 = data[['date','serial_number', 'model', 'failure']]\n",
    "        ListDF.append(data2)\n",
    "    \n",
    "    df = pd.concat(ListDF, ignore_index = True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "    \n",
    "def ExtractEntriesAndFailures (df) :   \n",
    "    '''Return dataframe with number of failures and unique serial numbers'''\n",
    "    FailuresPerModel = df.groupby('model')['failure'].sum()\n",
    "    EntriesPerModel = df.groupby('model')['serial_number'].unique()\n",
    "\n",
    "    ModelsDetail = pd.concat([EntriesPerModel,FailuresPerModel],axis=1)\n",
    "    return ModelsDetail\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# String needed to access the data in the zipfiles stored at https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data\n",
    "\n",
    "url_15 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_2015.zip'\n",
    "start_str_15 = '2015/'\n",
    "\n",
    "url_16_q1 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q1_2016.zip'\n",
    "start_str_16_q1 = 'data_Q1_2016/'\n",
    "\n",
    "url_16_q2 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q2_2016.zip'\n",
    "start_str_16_q2 = 'data_Q2_2016/'\n",
    "\n",
    "url_16_q3 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q3_2016.zip'\n",
    "start_str_16_q3 = 'data_Q3_2016/'\n",
    "\n",
    "url_16_q4 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q4_2016.zip'\n",
    "\n",
    "url_17_q1 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q1_2017.zip'\n",
    "\n",
    "url_17_q2 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q2_2017.zip'\n",
    "\n",
    "url_17_q3 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q3_2017.zip'\n",
    "\n",
    "url_17_q4 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q4_2017.zip'\n",
    "start_str_17_q4 = 'data_Q4_2017/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load data from 2015\n",
    "\n",
    "ModelsDetail_15 = ExtractEntriesAndFailures(ReadFiles_EntriesFailure(url_15, start_str_15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data from 2016\n",
    "\n",
    "df_16_q1 = ReadFiles_EntriesFailure(url_16_q1, start_str_16_q1)\n",
    "\n",
    "df_16_q2 = ReadFiles_EntriesFailure(url_16_q2, start_str_16_q2)\n",
    "\n",
    "df_16_q3 = ReadFiles_EntriesFailure(url_16_q3, start_str_16_q3)\n",
    "\n",
    "df_16_q4 = ReadFiles_EntriesFailure(url_16_q4)\n",
    "\n",
    "# Concatenate datasets of the 4 quartiles\n",
    "df = pd.concat([df_16_q1, df_16_q2, df_16_q3, df_16_q4], ignore_index = True)\n",
    "\n",
    "ModelsDetail_16 = ExtractEntriesAndFailures(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data from 2017\n",
    "\n",
    "df_17_q1 = ReadFiles_EntriesFailure(url_17_q1)\n",
    "\n",
    "df_17_q2 = ReadFiles_EntriesFailure(url_17_q2)\n",
    "\n",
    "df_17_q3 = ReadFiles_EntriesFailure(url_17_q3)\n",
    "\n",
    "df_17_q4 = ReadFiles_EntriesFailure(url_17_q4, start_str_17_q4)\n",
    "\n",
    "# Concatenate datasets of the 4 quartiles\n",
    "df = pd.concat([df_17_q1, df_17_q2, df_17_q3, df_17_q4], ignore_index = True)\n",
    "\n",
    "ModelsDetail_17 = ExtractEntriesAndFailures(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For every year we saved the number of failures and an array of unique serial numbers\n",
    "\n",
    "We combine the information across the three years in a common dataset. Observe that the same hard disk may have been used across different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ListDF_raw = [ModelsDetail_15.add_suffix('_15'), ModelsDetail_16.add_suffix('_16'), ModelsDetail_17.add_suffix('_17')]\n",
    "\n",
    "#df_raw = reduce(lambda x, y: pd.merge(x, y, on = 'model'), ListDF_raw)\n",
    "\n",
    "DfTot = pd.concat(ListDF_raw, axis = 1) \n",
    "\n",
    "DfTot.failure_15 = DfTot.failure_15.fillna(0) \n",
    "DfTot.failure_16 = DfTot.failure_16.fillna(0) \n",
    "DfTot.failure_17 = DfTot.failure_17.fillna(0) \n",
    "\n",
    "DfTot['failure_tot'] = DfTot.failure_15 + DfTot.failure_16 + DfTot.failure_17\n",
    "del DfTot['failure_15']\n",
    "del DfTot['failure_16']\n",
    "del DfTot['failure_17']\n",
    "\n",
    "DfTot['serial_number_tot'] = [np.hstack((DfTot.loc[mod,'serial_number_15'], DfTot.loc[mod,'serial_number_16'], DfTot.loc[mod,'serial_number_17'])) for mod in DfTot.index]\n",
    "\n",
    "del DfTot['serial_number_15']\n",
    "del DfTot['serial_number_16']\n",
    "del DfTot['serial_number_17']\n",
    "\n",
    "DfTot['entries_tot'] = [len(set(DfTot.loc[mod,'serial_number_tot'])) for mod in DfTot.index]\n",
    "\n",
    "del DfTot['serial_number_tot']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every model we have the number of distinct hard disks and the number of failures observed. \n",
    "We look at their ratio and filter the models for which too few hard disks have been sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>failure_tot</th>\n",
       "      <th>entries_tot</th>\n",
       "      <th>ratio_entry_failures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>WDC WD30EFRX</th>\n",
       "      <td>122.0</td>\n",
       "      <td>1261</td>\n",
       "      <td>0.096749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST3000DM001</th>\n",
       "      <td>106.0</td>\n",
       "      <td>1170</td>\n",
       "      <td>0.090598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST4000DM000</th>\n",
       "      <td>2590.0</td>\n",
       "      <td>36700</td>\n",
       "      <td>0.070572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST31500541AS</th>\n",
       "      <td>112.0</td>\n",
       "      <td>1693</td>\n",
       "      <td>0.066155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hitachi HDS723030ALA640</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1018</td>\n",
       "      <td>0.043222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hitachi HDS722020ALA330</th>\n",
       "      <td>152.0</td>\n",
       "      <td>4683</td>\n",
       "      <td>0.032458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST6000DX000</th>\n",
       "      <td>60.0</td>\n",
       "      <td>1938</td>\n",
       "      <td>0.030960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hitachi HDS5C3030ALA630</th>\n",
       "      <td>96.0</td>\n",
       "      <td>4608</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hitachi HDS5C4040ALE630</th>\n",
       "      <td>42.0</td>\n",
       "      <td>2660</td>\n",
       "      <td>0.015789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST8000DM002</th>\n",
       "      <td>141.0</td>\n",
       "      <td>10029</td>\n",
       "      <td>0.014059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HGST HMS5C4040ALE640</th>\n",
       "      <td>97.0</td>\n",
       "      <td>8661</td>\n",
       "      <td>0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HGST HMS5C4040BLE640</th>\n",
       "      <td>135.0</td>\n",
       "      <td>16306</td>\n",
       "      <td>0.008279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST8000NM0055</th>\n",
       "      <td>88.0</td>\n",
       "      <td>14510</td>\n",
       "      <td>0.006065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST10000NM0086</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1225</td>\n",
       "      <td>0.002449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST12000NM0007</th>\n",
       "      <td>17.0</td>\n",
       "      <td>7244</td>\n",
       "      <td>0.002347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         failure_tot  entries_tot  ratio_entry_failures\n",
       "WDC WD30EFRX                   122.0         1261              0.096749\n",
       "ST3000DM001                    106.0         1170              0.090598\n",
       "ST4000DM000                   2590.0        36700              0.070572\n",
       "ST31500541AS                   112.0         1693              0.066155\n",
       "Hitachi HDS723030ALA640         44.0         1018              0.043222\n",
       "Hitachi HDS722020ALA330        152.0         4683              0.032458\n",
       "ST6000DX000                     60.0         1938              0.030960\n",
       "Hitachi HDS5C3030ALA630         96.0         4608              0.020833\n",
       "Hitachi HDS5C4040ALE630         42.0         2660              0.015789\n",
       "ST8000DM002                    141.0        10029              0.014059\n",
       "HGST HMS5C4040ALE640            97.0         8661              0.011200\n",
       "HGST HMS5C4040BLE640           135.0        16306              0.008279\n",
       "ST8000NM0055                    88.0        14510              0.006065\n",
       "ST10000NM0086                    3.0         1225              0.002449\n",
       "ST12000NM0007                   17.0         7244              0.002347"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DfTot['ratio_entry_failures'] = DfTot['failure_tot'].divide(DfTot['entries_tot'], axis = 'rows')\n",
    "DfTot.sort_values('ratio_entry_failures', inplace=True, ascending =False)\n",
    "\n",
    "DfTot[DfTot['entries_tot'] > 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each brand has a different way of measuring the SMART metrics. For every model we need to understand which metrics are important and which are not. \n",
    "\n",
    "We now select a model and look at the data in the 2015 time frame. This is needed to obtain a mask that will be used to extract only the columns relevant to a specific model. \n",
    "\n",
    "Every model is associated with different SMART metrics. Moreover, for every relevant SMART metric we will work with the normalized value (in between 0 and 100/200/260 depending on the metric, the higher the better in tems of performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## The following functions are needed in order to build a mask for any specific model.\n",
    "\n",
    "# (1) Read the datase associated to a model and a specific mask.\n",
    "def ReadFiles_Model_and_RelCols (url, mod_name, rel_cols, start_string = ''):\n",
    "    ''' This function reads the zip file located at url, open it, read the columns associated to a model and \n",
    "    the relevant columns '''\n",
    "    ListDF = []\n",
    "    r = requests.get(url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    files = [name for name in z.namelist() if (name.endswith('.csv')) & (name.startswith(start_string))]\n",
    "    for file in files : \n",
    "        data = pd.read_csv(z.open(file), parse_dates = [0])\n",
    "        data2 = data[data['model'] == mod_name]\n",
    "        data3 = data2.iloc[:,rel_cols]\n",
    "        ListDF.append(data3)\n",
    "    \n",
    "    df = pd.concat(ListDF, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# (2) Return the SMART metrics with no NaN values and relevant variance.\n",
    "def RelevantCols(df, thresh = 20) :\n",
    "    ''' For the various SMART metrics return True only if the metric is considered by a specific model, i.e. the \n",
    "    relative values are not NaN and if the metric is relevant, i.e., the normalized range (max - min) is larger \n",
    "    than thresh'''\n",
    "    non_na_cols = (df.isnull().sum() == 0)\n",
    "        \n",
    "    res_bool = []\n",
    "        \n",
    "    for col in df.columns :\n",
    "        if non_na_cols[col] == True :\n",
    "            diff = df[col].max() - df[col].min()\n",
    "            if diff > thresh :\n",
    "                res_bool = res_bool + [True]\n",
    "            else :\n",
    "                res_bool = res_bool + [False]\n",
    "        else :\n",
    "            res_bool = res_bool + [False]\n",
    "    return res_bool\n",
    "    \n",
    "# (3) Build the mask given the result of RelevantCols    \n",
    "def BuildTheMask(rel_cols) :\n",
    "    ''' Esclude all raw columns and all the normalized columns not relevant for a specific model '''\n",
    "    \n",
    "    rel_cols_double = [val for val in rel_cols for _ in (0, 1)] \n",
    "    \n",
    "    return [a and b for a, b in zip(rel_cols_double, normalized_cols)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select the model you want to work with\n",
    "\n",
    "ModelName = 'Hitachi HDS722020ALA330' # (ex:'Hitachi HDS722020ALA330')\n",
    "\n",
    "## normalized_cols has 'False' in correspondence of a column with a raw value and 'True' of a col with a normalized value\n",
    "\n",
    "normalized_cols = [\n",
    "                True,  False, True,  False, True,  False,  True,  False, # SMART 1 - 2 - 3 - 4 \n",
    "                True,  False, True,  False, True,  False,  True,  False, # SMART 5 - 7 - 8 - 9 \n",
    "                True,  False, True,  False, True,  False,  True,  False, # SMART 10 - 11 - 12 -13\n",
    "                True,  False, True,  False, True,  False,  True,  False, # SMART 15 - 22 - 183 - 184 \n",
    "                True,  False, True,  False, True,  False,  True,  False, # SMART 187 - 188 - 189 - 190\n",
    "                True,  False, True,  False, True,  False,  True,  False, # SMART 191 - 192 - 193 - 194 \n",
    "                True,  False, True,  False, True,  False,  True,  False, # SMART 195 - 196 - 197 - 198 \n",
    "                True,  False, True,  False, True,  False,  True,  False, # SMART 199 - 200 - 201 - 220\n",
    "                True,  False, True,  False, True,  False,  True,  False, # SMART 222 - 223 - 224 - 225\n",
    "                True,  False, True,  False, True,  False,  True,  False, # SMART 226 - 240 - 241 - 242\n",
    "                True,  False, True,  False, True,  False,  True,  False, True,  False] # SMART 250 - 251 - 252 - 254 - 255\n",
    "\n",
    "# [date, serial number, model, capacity, failure] + SMART metrics\n",
    "cols = [False, False, False, False, False] + normalized_cols \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call (1) with the 2015 dataset and keeping all the normalized columns\n",
    "url_15 = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_2015.zip'\n",
    "start_str_15 = '2015/'\n",
    "df15 = ReadFiles_Model_and_RelCols(url_15, ModelName, cols, start_string = start_str_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call (2) with the dataset df15 obtained with the previous call\n",
    "relevant_cols = RelevantCols(df15) \n",
    "\n",
    "# Call (3) with the relevant columns relevant_cols    \n",
    "mask = BuildTheMask(relevant_cols)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have the mask relative to the model indicated in ModelName (ex:'Hitachi HDS722020ALA330'). \n",
    "\n",
    "We access the online data once again and save only the rows related to ModelName and the columns specified by the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [date, serial number, model, capacity, failure] + SMART metrics\n",
    "cols = [True, True, False, False, True] + normalized_cols \n",
    "\n",
    "# Path to the folder in which the datset will be stored\n",
    "folder_name_model = ModelName.replace(' ', '_')\n",
    "folder_name = 'Data/' + folder_name_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save data of 2015\n",
    "\n",
    "df_15 = ReadFiles_Model_and_RelCols(url_15, ModelName, cols, start_string = start_str_15)\n",
    "df_15.to_csv(folder_name + '/Model_15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save data of 2016\n",
    "\n",
    "df_16_q1 = ReadFiles_Model_and_RelCols(url_16_q1, ModelName, cols, start_str_16_q1)\n",
    "\n",
    "df_16_q2 = ReadFiles_Model_and_RelCols(url_16_q2, ModelName, cols, start_str_16_q2)\n",
    "\n",
    "df_16_q3 = ReadFiles_Model_and_RelCols(url_16_q3, ModelName, cols, start_str_16_q3)\n",
    "\n",
    "df_16_q4 = ReadFiles_Model_and_RelCols(url_16_q4, ModelName, cols)\n",
    "\n",
    "# Concatenate datasets of the 4 quartiles\n",
    "df_16 = pd.concat([df_16_q1, df_16_q2, df_16_q3, df_16_q4], ignore_index = True)\n",
    "\n",
    "df_16.to_csv(folder_name + '/Model_16.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save data of 2017\n",
    "\n",
    "df_17_q1 = ReadFiles_Model_and_RelCols(url_17_q1, ModelName, cols)\n",
    "\n",
    "df_17_q2 = ReadFiles_Model_and_RelCols(url_17_q2, ModelName, cols)\n",
    "\n",
    "df_17_q3 = ReadFiles_Model_and_RelCols(url_17_q3, ModelName, cols)\n",
    "\n",
    "df_17_q4 = ReadFiles_Model_and_RelCols(url_17_q4, ModelName, cols, start_str_17_q4)\n",
    "\n",
    "# Concatenate datasets of the 4 quartiles\n",
    "df_17 = pd.concat([df_17_q1, df_17_q2, df_17_q3, df_17_q4], ignore_index = True)\n",
    "\n",
    "df_17.to_csv(folder_name + '/Model_17.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
