{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we predict a failure by looking at the SMART metrics ?\n",
    "\n",
    "### The goal is to sustain planned checking and controls (offline) with predictive maintenance (online). These predictions should depend on the evolution of the SMART metrics, but how do we know whether fluctuations of these metrics and failures are correlated?\n",
    "\n",
    "After running the notebook \"DataWrangling_1_Models\", we have a folder with data associated to a specific model, ex: 'Hitachi HDS722020ALA330'. We read those files and explore the dataframes.\n",
    "\n",
    "Note: The dataset we analyze is manageable with a 4-Core, 8GB Ram personal laptop for every hard disk model but 'ST4000DM000' which would require a larger memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>failure</th>\n",
       "      <th>smart_1_normalized</th>\n",
       "      <th>smart_2_normalized</th>\n",
       "      <th>smart_3_normalized</th>\n",
       "      <th>smart_5_normalized</th>\n",
       "      <th>smart_8_normalized</th>\n",
       "      <th>smart_192_normalized</th>\n",
       "      <th>smart_193_normalized</th>\n",
       "      <th>smart_194_normalized</th>\n",
       "      <th>smart_196_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.608197e+06</td>\n",
       "      <td>2.608197e+06</td>\n",
       "      <td>2.608197e+06</td>\n",
       "      <td>2.608197e+06</td>\n",
       "      <td>2.608197e+06</td>\n",
       "      <td>2.608197e+06</td>\n",
       "      <td>2.608197e+06</td>\n",
       "      <td>2.608197e+06</td>\n",
       "      <td>2.608197e+06</td>\n",
       "      <td>2.608197e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.827781e-05</td>\n",
       "      <td>9.970960e+01</td>\n",
       "      <td>1.030739e+02</td>\n",
       "      <td>1.186466e+02</td>\n",
       "      <td>9.995405e+01</td>\n",
       "      <td>1.019221e+02</td>\n",
       "      <td>9.969824e+01</td>\n",
       "      <td>9.969824e+01</td>\n",
       "      <td>2.067698e+02</td>\n",
       "      <td>9.995118e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.633769e-03</td>\n",
       "      <td>1.643947e+00</td>\n",
       "      <td>9.510941e+00</td>\n",
       "      <td>6.701817e+00</td>\n",
       "      <td>1.190153e+00</td>\n",
       "      <td>6.035827e+00</td>\n",
       "      <td>6.207148e-01</td>\n",
       "      <td>6.207148e-01</td>\n",
       "      <td>3.054345e+01</td>\n",
       "      <td>1.220344e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>4.600000e+01</td>\n",
       "      <td>6.800000e+01</td>\n",
       "      <td>6.800000e+01</td>\n",
       "      <td>1.150000e+02</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.150000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>1.870000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.160000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>2.060000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.200000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>2.300000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.350000e+02</td>\n",
       "      <td>2.530000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.230000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>2.530000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            failure  smart_1_normalized  smart_2_normalized  \\\n",
       "count  2.608197e+06        2.608197e+06        2.608197e+06   \n",
       "mean   5.827781e-05        9.970960e+01        1.030739e+02   \n",
       "std    7.633769e-03        1.643947e+00        9.510941e+00   \n",
       "min    0.000000e+00        1.000000e+00        1.000000e+02   \n",
       "25%    0.000000e+00        1.000000e+02        1.000000e+02   \n",
       "50%    0.000000e+00        1.000000e+02        1.000000e+02   \n",
       "75%    0.000000e+00        1.000000e+02        1.000000e+02   \n",
       "max    1.000000e+00        1.000000e+02        1.350000e+02   \n",
       "\n",
       "       smart_3_normalized  smart_5_normalized  smart_8_normalized  \\\n",
       "count        2.608197e+06        2.608197e+06        2.608197e+06   \n",
       "mean         1.186466e+02        9.995405e+01        1.019221e+02   \n",
       "std          6.701817e+00        1.190153e+00        6.035827e+00   \n",
       "min          1.000000e+02        5.000000e+00        4.600000e+01   \n",
       "25%          1.150000e+02        1.000000e+02        1.000000e+02   \n",
       "50%          1.160000e+02        1.000000e+02        1.000000e+02   \n",
       "75%          1.200000e+02        1.000000e+02        1.000000e+02   \n",
       "max          2.530000e+02        1.000000e+02        1.230000e+02   \n",
       "\n",
       "       smart_192_normalized  smart_193_normalized  smart_194_normalized  \\\n",
       "count          2.608197e+06          2.608197e+06          2.608197e+06   \n",
       "mean           9.969824e+01          9.969824e+01          2.067698e+02   \n",
       "std            6.207148e-01          6.207148e-01          3.054345e+01   \n",
       "min            6.800000e+01          6.800000e+01          1.150000e+02   \n",
       "25%            9.900000e+01          9.900000e+01          1.870000e+02   \n",
       "50%            1.000000e+02          1.000000e+02          2.060000e+02   \n",
       "75%            1.000000e+02          1.000000e+02          2.300000e+02   \n",
       "max            1.000000e+02          1.000000e+02          2.530000e+02   \n",
       "\n",
       "       smart_196_normalized  \n",
       "count          2.608197e+06  \n",
       "mean           9.995118e+01  \n",
       "std            1.220344e+00  \n",
       "min            4.000000e+00  \n",
       "25%            1.000000e+02  \n",
       "50%            1.000000e+02  \n",
       "75%            1.000000e+02  \n",
       "max            1.000000e+02  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset associated to a specific model:  (ex:'Hitachi HDS722020ALA330', 'ST8000DM002')\n",
    "\n",
    "#ModelName = 'ST8000DM002' \n",
    "ModelName = 'Hitachi HDS722020ALA330' \n",
    "\n",
    "folder_name_model = ModelName.replace(' ', '_')\n",
    "folder_name = 'Data/' + folder_name_model\n",
    "\n",
    "df15 = pd.read_csv(folder_name + '/Model_15.csv')\n",
    "df16 = pd.read_csv(folder_name + '/Model_16.csv')\n",
    "df17 = pd.read_csv(folder_name + '/Model_17.csv')\n",
    "\n",
    "df15['date'] = pd.to_datetime(df15['date'])\n",
    "df16['date'] = pd.to_datetime(df16['date'])\n",
    "df17['date'] = pd.to_datetime(df17['date'])\n",
    "\n",
    "# Concatenate the yearly datasets in an unique pandas DataFrame : df\n",
    "\n",
    "df = pd.concat([df15,df16,df17], ignore_index=True)\n",
    "\n",
    "del df15\n",
    "del df16\n",
    "del df17\n",
    "\n",
    "del df['Unnamed: 0']  #### Why this column is there?\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_first</th>\n",
       "      <th>date_last</th>\n",
       "      <th>date_failure</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>JK1101B9JPJ4NF</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>2015-03-30</td>\n",
       "      <td>wrong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date_first  date_last date_failure  state\n",
       "JK1101B9JPJ4NF 2015-01-01 2016-08-04   2015-03-30  wrong"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the data associated to each hard disk: whether it will fail, the first, and the last entry   \n",
    "\n",
    "df_failed = df.groupby('serial_number').failure.sum()\n",
    "df_last_app = df.groupby('serial_number').date.max()\n",
    "df_first_app = df.groupby('serial_number').date.min()\n",
    "\n",
    "\n",
    "# Data in the days of failures : verify that the last entry of a failing hard-disk coincides with a failure\n",
    "df_failure_days = df.loc[df['failure']==1,['date','serial_number']]\n",
    "df_failure_days.index = df_failure_days['serial_number']\n",
    "\n",
    "del df_failure_days['serial_number']\n",
    "\n",
    "# Generate a DataFrame with data associated to each serial number    \n",
    "df_serialnumber = pd.concat([df_failed, df_first_app, df_last_app, df_failure_days], axis = 1)\n",
    "df_serialnumber.columns = ['failure', 'date_first', 'date_last', 'date_failure']\n",
    "\n",
    "# ... check whether the last entry of a failing hard-disk coincides with a failure\n",
    "df_serialnumber['failure_wrong'] = df_serialnumber.failure * (df_serialnumber.date_last != df_serialnumber.date_failure)\n",
    "df_serialnumber['failure_correct'] = df_serialnumber.failure * (1 - df_serialnumber.failure_wrong)\n",
    "\n",
    "del df_failed\n",
    "del df_last_app\n",
    "del df_first_app\n",
    "del df_failure_days\n",
    "\n",
    "del df_serialnumber['failure_wrong']\n",
    "\n",
    "# We now determine whether each serial_number is going to fail, will remain healthy, or is a wrong entry\n",
    "dictionary_serial_number_state ={ (1,1): 'fail',\n",
    "                                  (0,0): 'healthy',\n",
    "                                  (1,0): 'wrong' }\n",
    "\n",
    "df_serialnumber['state'] = [ dictionary_serial_number_state[(df_serialnumber.loc[s_n,'failure'],df_serialnumber.loc[s_n,'failure_correct'])] for s_n in df_serialnumber.index]\n",
    "\n",
    "del df_serialnumber['failure']\n",
    "del df_serialnumber['failure_correct']\n",
    "\n",
    "# In the example with ModelName = 'Hitachi HDS722020ALA330', we have one wrongly reported hard-disk: 'JK1101B9JPJ4NF' \n",
    "df_serialnumber[df_serialnumber['state'] == 'wrong']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add a column to the main df stating whether the entry belongs to a failing hard-disk: 1 or not: 0\n",
    "df['state'] = df.serial_number.apply(lambda x : df_serialnumber.state[x])\n",
    "\n",
    "# The dataset has a natural structure with a 2-levels indexing :\n",
    "# 1) The outer level captures a specific hard-disk (identified by the serial_number) ;\n",
    "# 2) The inner level captures the TimeStamp of the recording. \n",
    "\n",
    "df = df.set_index(['serial_number','date']).sort_index()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# smart_metrics contains the list of names for the SMART metrics we are considering for the model ModelName\n",
    "smart_metrics = df.columns[1:-1]\n",
    "\n",
    "def col_info(x):\n",
    "    '''This function returns a Pandas Series with info associated to each row'''\n",
    "    return pd.Series(index=['min','max','mean','std'],data=[x.min(),x.max(),x.mean(),x.std()])\n",
    "\n",
    "# df_smartmetrics_minmax is a dataframe with the minimum and the maximum for each of these metrics.\n",
    "df_smartmetrics = df[smart_metrics].apply(col_info)\n",
    "\n",
    "df_smartmetrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_failing = df[df['state'] == 'fail']  # We filter the hard-disk failing in the considered time frame\n",
    "df_healthy = df[df['state'] == 'healthy'] # ... and those which do not\n",
    "\n",
    "# We now extract the SMART metrics associated to hard-disks in their last day of life : df_failure\n",
    "\n",
    "df_failure = df_failing.groupby(level='serial_number').tail(1)\n",
    "\n",
    "# ... and in the K days before the failure : df_failure_almost\n",
    "\n",
    "K = 7   # K = 7 means we look at the week prior to the failure\n",
    "\n",
    "df_failure_K = df_failing.groupby(level='serial_number').tail(K)\n",
    "\n",
    "del df_failing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have wrangled the data and now have the following cleaned DataFrames :\n",
    "\n",
    "### 1) df : This is our main dataset, it contains all the information from the original dataframe.\n",
    "\n",
    "df_healthy : df filtered, keeping only the data associated to non-failing hard-disks.\n",
    "\n",
    "df_failure : df filtered, keeping only the data associated to failing hard-disks in their last day.\n",
    "\n",
    "df_failure_K : df filtered, keeping only the data associated to failing hard-disks in their last K days.\n",
    "\n",
    "### 2) df_serialnumber : This contains the info associated to each hard-disk (grouped by serial_number).\n",
    "\n",
    "### 3) df_smartmetrics : This contains the info associated to each smart metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis 1 :\n",
    "\n",
    "### We look at the SMART metrics of hard-disks in healthy state (df_healthy), of hard-disks close to failure (df_failure_K), and of hard-disks in their last day of work (df_failure).\n",
    "\n",
    "### A few working questions :\n",
    "\n",
    "1) For a chosen SMART metric, does its distribution d_healthy, d_K, and d_fail change across the different groups of hard-disks?\n",
    "\n",
    "2) If d_fail = d_healthy we might want to discard the SMART metric, it does not provide information on whether an hard-disk is approaching failure.\n",
    "\n",
    "3) If d_fail is different from d_healthy, we should keep the SMART metric. Note that for K=1 we have that d_K = d_fail and for K-> inf we have d_K = d_healthy. How can we set an optimal K? \n",
    "\n",
    "4) Note that is K is large we might be able to capture the malfunction of the hard-disk well in advance and be able to act accordingly, if K is small we might not have time to replace the hard-disk before its failure. Hence, the window length K should be as large as possible while allowing to discriminate d_K and d_healthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider example with ModelName = 'Hitachi HDS722020ALA330', we deduce that:\n",
    "\n",
    "1) SMART 192 and SMART 193 are identical and one of them can be dropped.\n",
    "\n",
    "2) SMART 2 and SMART 194 are unlikely to be useful in predicting failures: the distributions are very similar for healthy and failing hard disks.\n",
    "\n",
    "3) SMART metrics 1, 5, 8, 196 seem to be the most useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The function CompareHistograms plots multi-bar histograms of a list of np arrays. It sets the right value for the width\n",
    "# and the position of the bar according to the range and the number of bins.\n",
    "\n",
    "def compare_histograms (*list_array, hist_range, num_bins = 10) :\n",
    "    '''Create a histogram with multiple bars from a list of arrays. Each array is plotted across hist_range'''\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    \n",
    "    # Initialize the width of each bar\n",
    "    width_0 = ((hist_range[1]-hist_range[0])/num_bins) / (len(list_array)+1)\n",
    "    \n",
    "    for ind, arr in enumerate(list_array) :\n",
    "        \n",
    "        heights_arr , bins_arr = np.histogram(arr, normed=True, range = hist_range, bins = num_bins)\n",
    "        ax.bar(bins_arr[:-1] +  ((hist_range[1]-hist_range[0])/num_bins)/2 + width_0*(ind + 1), heights_arr, width=width_0) \n",
    "\n",
    "    return fig,ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We now apply CompareHistograms to every SMART metric and plot the multi-bar histograms\n",
    "\n",
    "for smart_metric in smart_metrics :\n",
    "\n",
    "    dfh1 = df_failure.loc[(slice(None),slice(None)), smart_metric].values\n",
    "    dfh2 = df_failure_K.loc[(slice(None),slice(None)), smart_metric].values\n",
    "    dfh3 = df_healthy.loc[(slice(None),slice(None)), smart_metric].values\n",
    "\n",
    "    fig, ax = compare_histograms(dfh1,dfh2,dfh3, hist_range = (df_smartmetrics.loc['min',smart_metric],df_smartmetrics.loc['max',smart_metric]) )\n",
    "    \n",
    "    plt.xlabel(smart_metric)\n",
    "    plt.ylabel('logscale normalized hist')\n",
    "    \n",
    "    plt.yscale('log')\n",
    "    plt.legend(['failed','K = %s' % K,'healthy'])\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis 2 :\n",
    "\n",
    "### We look at the evolution of the SMART metrics as the hard disks approaches the failure.\n",
    "\n",
    "### A few working questions :\n",
    "\n",
    "1) Does the SMART metrics worsen abruptly or smoothly as the failure approach?  \n",
    "\n",
    "2) How many hard-disks failures can be predicted for each SMART metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Consider example with ModelName = 'Hitachi HDS722020ALA330', we deduce that:\n",
    "\n",
    "1) SMART 1, 5, and 196 metrics drop about 15 days prior to the failure of an hard disk.\n",
    "\n",
    "2) Roughly 1% (SMART 1) and 5% (SMART 5 and 196) of the hard disk failures can be predicted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_failing = df[df['state'] == 'fail'].reset_index()\n",
    "\n",
    "del df_failing['state']\n",
    "\n",
    "df_failing['days_to_failure'] = [ (df_serialnumber.date_last[df_failing.iloc[i,0]] - df_failing.iloc[i,1]).days for i in range(df_failing.shape[0])]\n",
    "\n",
    "del df_failing['date']\n",
    "del df_failing['failure']\n",
    "\n",
    "df_failing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perc_min = df_failing.groupby('days_to_failure')[smart_metrics].min()\n",
    "perc_005 = df_failing.groupby('days_to_failure')[smart_metrics].quantile(.005)\n",
    "perc_010 = df_failing.groupby('days_to_failure')[smart_metrics].quantile(.01)\n",
    "perc_050 = df_failing.groupby('days_to_failure')[smart_metrics].quantile(.05)\n",
    "perc_100 = df_failing.groupby('days_to_failure')[smart_metrics].quantile(.1)\n",
    "perc_500 = df_failing.groupby('days_to_failure')[smart_metrics].quantile(.5)\n",
    "perc_max = df_failing.groupby('days_to_failure')[smart_metrics].max()\n",
    "\n",
    "# We now apply CompareHistograms to every SMART metric and plot the multi-bar histograms\n",
    "\n",
    "for smart_metric in smart_metrics :\n",
    "\n",
    "    plt.plot(perc_min[smart_metric], marker = 'o', linestyle = '--')\n",
    "    plt.plot(perc_005[smart_metric], marker = 'o', linestyle = '--')\n",
    "    plt.plot(perc_010[smart_metric], marker = 'o', linestyle = '--')\n",
    "    plt.plot(perc_050[smart_metric], marker = 'o', linestyle = '--')\n",
    "    plt.plot(perc_100[smart_metric], marker = 'o', linestyle = '--')\n",
    "    plt.plot(perc_500[smart_metric], marker = 'o', linestyle = '--')\n",
    "    plt.plot(perc_max[smart_metric], marker = 'o', linestyle = '--')\n",
    "\n",
    "    plt.axis( [0, 40, min(perc_min[smart_metric]) - 2, max(perc_max[smart_metric]) + 2] )\n",
    "    plt.xlabel('Days to failure')\n",
    "    plt.ylabel(smart_metric)\n",
    "    plt.legend(['minimum',\n",
    "                'quantile .005',\n",
    "                'quantile .01',\n",
    "                'quantile .05',\n",
    "                'quantile .1',\n",
    "                'median',\n",
    "                'maximum'])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis 3 :\n",
    "\n",
    "### We look at the evolution of the SMART metrics and their correlation both with age of the hard-disk and time to its failure\n",
    "\n",
    "### A few working questions :\n",
    "\n",
    "1) Does the SMART metrics worsen as the time passes? Is age a factor in the outcome of the SMART metrics? \n",
    "\n",
    "2) Are the hard-disk age and the smart metrics correlated?\n",
    "\n",
    "3) Are the days to failure and the smart metrics correlated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Consider example with ModelName = 'Hitachi HDS722020ALA330', we deduce that:\n",
    "\n",
    "1) Consider the age of the hard-disk: There is a clear inverse correlation betwen age of an hard-disk and SMART 3 (about -0.3). The other metrics seem incorrelated.\n",
    "\n",
    "2) Consider the time-to-failure of the hard-disk: SMART 1, 5, and 196 have a mild positive correlation with the time to failure (between 0.05 and 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_reset = df.reset_index()\n",
    "df_present = df_reset[df_reset['date'] == '2015-01-01']\n",
    "\n",
    "set_of_present_serialnumbers = set(df_present['serial_number'].values)\n",
    "\n",
    "df_reset['present'] = df_reset.serial_number.apply(lambda x : x in set_of_present_serialnumbers)\n",
    "\n",
    "df_nonpresent = df_reset[df_reset['present'] == False].reset_index()\n",
    "\n",
    "df_nonpresent['days_active'] = [ (df_nonpresent.iloc[i,2] - df_serialnumber.date_first[df_nonpresent.iloc[i,1]]).days for i in range(df_nonpresent.shape[0])]\n",
    "\n",
    "df_nonpresent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_corr = {}\n",
    "\n",
    "for smart_metric in smart_metrics :\n",
    "\n",
    "    df_aux = df_nonpresent[[smart_metric, 'days_active']]\n",
    "    corr = df_aux.corr(method='pearson')\n",
    "    \n",
    "    dict_corr[smart_metric] = corr.iloc[0,1]\n",
    "        \n",
    "df_corr = pd.DataFrame.from_dict(dict_corr, orient = 'index').transpose()   \n",
    "df_corr_age = df_corr.rename(index={0: 'corr_age'})\n",
    "\n",
    "dict_corr = {}\n",
    "\n",
    "for smart_metric in smart_metrics :\n",
    "\n",
    "    df_aux = df_failing[[smart_metric, 'days_to_failure']]\n",
    "    corr = df_aux.corr(method='pearson')\n",
    "    \n",
    "    dict_corr[smart_metric] = corr.iloc[0,1]\n",
    "        \n",
    "df_corr = pd.DataFrame.from_dict(dict_corr, orient = 'index').transpose()   \n",
    "df_corr_time_to_fail = df_corr.rename(index={0: 'corr_time_to_fail'})\n",
    "\n",
    "df_smartmetrics = pd.concat([df_smartmetrics, df_corr_age, df_corr_time_to_fail])\n",
    "df_smartmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perc_min = df_nonpresent.groupby('days_active')[smart_metrics].min()\n",
    "perc_005 = df_nonpresent.groupby('days_active')[smart_metrics].quantile(.005)\n",
    "perc_010 = df_nonpresent.groupby('days_active')[smart_metrics].quantile(.01)\n",
    "perc_050 = df_nonpresent.groupby('days_active')[smart_metrics].quantile(.05)\n",
    "perc_100 = df_nonpresent.groupby('days_active')[smart_metrics].quantile(.1)\n",
    "perc_500 = df_nonpresent.groupby('days_active')[smart_metrics].quantile(.5)\n",
    "perc_max = df_nonpresent.groupby('days_active')[smart_metrics].max()\n",
    "\n",
    "# We now apply CompareHistograms to every SMART metric and plot the multi-bar histograms\n",
    "\n",
    "for smart_metric in smart_metrics :\n",
    "\n",
    "    plt.plot(perc_min[smart_metric], marker = 'o', linestyle = '--')\n",
    "    plt.plot(perc_005[smart_metric], marker = 'o', linestyle = '--')\n",
    "    plt.plot(perc_010[smart_metric], marker = 'o', linestyle = '--')\n",
    "    plt.plot(perc_050[smart_metric], marker = 'o', linestyle = '--')\n",
    "    plt.plot(perc_100[smart_metric], marker = 'o', linestyle = '--')\n",
    "    plt.plot(perc_500[smart_metric], marker = 'o', linestyle = '--')\n",
    "    plt.plot(perc_max[smart_metric], marker = 'o', linestyle = '--')\n",
    "\n",
    "    plt.axis( [0, 500, min(perc_min[smart_metric]) - 2, max(perc_max[smart_metric]) + 2] )\n",
    "    plt.xlabel('age of the hard disk')\n",
    "    plt.ylabel(smart_metric)\n",
    "    plt.legend(['minimum',\n",
    "                'quantile .005',\n",
    "                'quantile .01',\n",
    "                'quantile .05',\n",
    "                'quantile .1',\n",
    "                'median',\n",
    "                'maximum'])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploratory data analysis 4 :\n",
    "\n",
    "### What can we say about the 'wrong entries ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wrong_sn = df_serialnumber[df_serialnumber['state'] == 'wrong'].index\n",
    "\n",
    "df_wrong = df.loc[wrong_sn].reset_index()\n",
    "\n",
    "df_wrong['days_to_failure'] = [ (df_serialnumber.date_failure[df_wrong.iloc[i,0]] - df_wrong.iloc[i,1]).days for i in range(df_wrong.shape[0])]\n",
    "\n",
    "for smart_metric in smart_metrics :\n",
    "\n",
    "    plt.plot(df_wrong['days_to_failure'],df_wrong[smart_metric], marker = 'o', linestyle = '--')\n",
    "\n",
    "    plt.xlabel('Days to failure')\n",
    "    plt.ylabel(smart_metric)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
